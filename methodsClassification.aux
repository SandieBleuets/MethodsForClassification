\relax 
\citation{bonaccorso2017machine}
\citation{joshi2017artificial}
\citation{dangeti2017statistics}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem formulation}{1}\protected@file@percent }
\newlabel{Formulation}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Data for classification summary (left), supplemented by a sleep state dataset example (right).}}{1}\protected@file@percent }
\newlabel{dataForm}{{1}{1}}
\citation{tang2014feature}
\citation{mitData}
\citation{coelho2015building}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Overview of the classification process from data collection to deployment.}}{2}\protected@file@percent }
\newlabel{processML}{{2}{2}}
\@writefile{toc}{\contentsline {paragraph}{Collection of data}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feature engineering}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning phase}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Testing the algorithm on unseen data}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deployment of the algorithm}{2}\protected@file@percent }
\citation{tang2014feature}
\citation{dash1997feature}
\@writefile{toc}{\contentsline {section}{\numberline {2}Dimensionality reduction}{3}\protected@file@percent }
\newlabel{dimension}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Overview of methods for classification.}}{3}\protected@file@percent }
\newlabel{mindmapML}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Feature selection}{3}\protected@file@percent }
\citation{liu2007computational}
\citation{duda2012pattern}
\citation{gu2012generalized}
\citation{bonaccorso2017machine}
\citation{roobaert2006information}
\citation{kira1992feature}
\citation{hall1999feature}
\citation{tang2014feature}
\citation{guyon2003introduction}
\citation{kohavi1997wrappers}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Framework of feature selection methods.}}{4}\protected@file@percent }
\newlabel{processFeatureSelection}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Filter methods}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Wrapper methods}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Feature extraction}{4}\protected@file@percent }
\citation{tang2014feature}
\citation{jolliffe2011principal}
\citation{neal2006high}
\citation{le2010multiple}
\citation{escofier2008analyses}
\citation{scholkopf1997kernel}
\citation{bonaccorso2017machine}
\citation{balakrishnama1998linear}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Feature extraction concept (left) and associated example (right). $X_i$ is the original feature set describing the sample $i$ and $Z_i$ is the extracted feature set. In the example, black dots represent data in a two-dimensional space and the red line represents an example of axis for projection in a one-dimensional space.}}{5}\protected@file@percent }
\newlabel{FeatureExtraction}{{5}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Principal Component Analysis}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Linear Discriminant Analysis}{5}\protected@file@percent }
\newlabel{LDAExtr}{{2.2.2}{5}}
\citation{rosenblatt1958perceptron}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of data projection between Principal Components Analysis and Linear Discriminant Analysis for a two-classes problem. Orange dots represent data of the first class and data belonging to the second class are reported in black. The red line represents the resulting axis for projection obtained for each method.}}{6}\protected@file@percent }
\newlabel{CompLDAPCA}{{6}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Machine learning algorithms}{6}\protected@file@percent }
\newlabel{MLtech}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Illustration of a linearly separable case (left) and a non-linearly separable case (right). Red lines represent examples of boundary decisions.}}{6}\protected@file@percent }
\newlabel{LinearVsNonLinear}{{7}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear algorithms}{6}\protected@file@percent }
\citation{shiffman2012nature}
\citation{czepiel2002maximum}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Perceptron}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Illustration of a perceptron with two inputs.}}{7}\protected@file@percent }
\newlabel{Perceptron}{{8}{7}}
\newlabel{linearEquation}{{3}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Logistic Regression}{7}\protected@file@percent }
\citation{rao1948tests}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Sigmoid function.}}{8}\protected@file@percent }
\newlabel{sigmoid}{{9}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Linear Discriminant Analysis}{8}\protected@file@percent }
\newlabel{LDAML}{{3.1.3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Non-linear algorithms}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}K-Nearest Neighbors}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Example of K-nearest neighbors classification with $k=3$.}}{9}\protected@file@percent }
\newlabel{KNNEx}{{10}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Decision tree}{9}\protected@file@percent }
\newlabel{DT}{{3.2.2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Tree architecture for a four classes examples, where $x_{i1}$, $x_{i3}$ and $x_{i3}$ are the feature set of a sample $i$ and $y_i$ is the predicted class. $T_1$, $T_2$, $T_3$ are three thresholds.}}{9}\protected@file@percent }
\newlabel{TreeEx}{{11}{9}}
\citation{dangeti2017statistics}
\citation{breiman2001random}
\citation{cortes1995support}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Random Forests}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Support Vector Machines}{10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Prediction workflow with a Random Forest model composed by $E$ trees. Successive test results for each tree are reported in green.}}{11}\protected@file@percent }
\newlabel{RFEx}{{12}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Example of a linear boundary estimated by SVM for a two class problem.}}{11}\protected@file@percent }
\newlabel{SVMEx}{{13}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Example of kernel transformation $\phi $ and boundary estimation (in red) with SVM.}}{11}\protected@file@percent }
\newlabel{KernelEx}{{14}{11}}
\citation{parizeau2004perceptron}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Multi-Layer Perceptron}{12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Example of multi-layer perceptron with 2 hidden layers and two output classes.}}{12}\protected@file@percent }
\newlabel{MLPEx}{{15}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Techniques for performance evaluation}{13}\protected@file@percent }
\newlabel{Evaluation}{{4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Metrics}{13}\protected@file@percent }
\newlabel{metrics}{{4.1}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Confusion matrix for a two class classification.}}{13}\protected@file@percent }
\newlabel{MC}{{16}{13}}
\citation{dangeti2017statistics}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Cross-validation}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Hold-out}{14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Illustration of the hold-out strategy.}}{14}\protected@file@percent }
\newlabel{holdout}{{17}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}k-fold cross-validation}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Illustration of the 3-fold cross-validation strategy.}}{15}\protected@file@percent }
\newlabel{kfold}{{18}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Leave-one-out cross-validation}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Illustration of the leave-one-out cross-validation strategy for a training/validation dataset of 20 samples.}}{15}\protected@file@percent }
\newlabel{LOOCV}{{19}{15}}
\bibstyle{acm}
\bibdata{chapterMethods}
\bibcite{balakrishnama1998linear}{1}
\bibcite{bonaccorso2017machine}{2}
\bibcite{breiman2001random}{3}
\bibcite{coelho2015building}{4}
\bibcite{cortes1995support}{5}
\bibcite{czepiel2002maximum}{6}
\bibcite{dangeti2017statistics}{7}
\bibcite{dash1997feature}{8}
\bibcite{duda2012pattern}{9}
\bibcite{escofier2008analyses}{10}
\bibcite{mitData}{11}
\bibcite{gu2012generalized}{12}
\bibcite{guyon2003introduction}{13}
\bibcite{hall1999feature}{14}
\bibcite{jolliffe2011principal}{15}
\bibcite{joshi2017artificial}{16}
\bibcite{kira1992feature}{17}
\bibcite{kohavi1997wrappers}{18}
\bibcite{le2010multiple}{19}
\bibcite{liu2007computational}{20}
\bibcite{neal2006high}{21}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Bibliography}{16}\protected@file@percent }
\bibcite{parizeau2004perceptron}{22}
\bibcite{rao1948tests}{23}
\bibcite{roobaert2006information}{24}
\bibcite{rosenblatt1958perceptron}{25}
\bibcite{scholkopf1997kernel}{26}
\bibcite{shiffman2012nature}{27}
\bibcite{tang2014feature}{28}
